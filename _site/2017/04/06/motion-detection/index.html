<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Imagination is more important than knowledge. -Albert Einstein">
    <meta name="keywords"  content="华俊豪, huajh, huajh7, Junhao Hua, @huajh, 华俊豪的博客, huajh7's Blog, 博客, 机器学习, 贝叶斯">
    <meta name="theme-color" content="#000000">
    
    <title>运动检测文献综述 - Junhao Hua 博客 | huajh7's Blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2017/04/06/motion-detection/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://huajh7.com" > Academic Website</a>
            <a class="navbar-brand" href="/">huajh7's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">about</a>
                    </li>
                    
                    <li>
                        <a href="/archives/">Archives</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-universe.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-universe.jpg')
    }

    
</style>
<header class="intro-header" >              
    <div class="header-mask"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#Motion Detection" title="Motion Detection">Motion Detection</a>
                        
                        <a class="tag" href="/tags/#Computer Vision" title="Computer Vision">Computer Vision</a>
                        
                    </div>
                    <h1>运动检测文献综述</h1>
                    
                    
                    <h2 class="subheading">Motion Detection, 运动侦测，移动侦测，移动检测</h2>
                    
                    <span class="meta">Posted by huajh7 on April 6, 2017</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>

    <div class="container">

        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-9 col-lg-offset-1
                col-md-10 col-md-offset-1
                post-container"> <!-- style="width:70%"  col-lg-9 col-lg-offset-1  col-lg-8 col-lg-offset-2 -->

				<blockquote>
  <p>又名：运动侦测，移动侦测，移动检测</p>
</blockquote>

<h2 id="方法和思路">方法和思路</h2>

<h3 id="1-帧差分法-frame-differencing">1. 帧差分法 (frame differencing)</h3>
<blockquote>
  <p>Frame differencing is a technique where the computer checks the differencebetween two video frames. If the pixels have changed there apparently was something changing in the image (moving for example). Most techniques work with some blur and threshold, to distict real movement from noise. Because frame could differ too when light conditions in a room change ( and <strong>camera auto focus</strong>, <strong>brightness correction</strong> etc. ).</p>
</blockquote>

<blockquote>
  <p>from: <a href="http://www.kasperkamperman.com/blog/computer-vision/computervision-framedifferencing/">kasperkamperman-computervision-framedifferencing</a></p>
</blockquote>

<ul>
  <li>帧间差分法</li>
  <li>三帧差分法</li>
</ul>

<blockquote>
  <p>背景差分法(Background difference) : 视频帧图像与背景模型图像进行差分和阈值分割</p>

  <p>帧差分法： 视频中的一帧图像与另一帧图像进行差分运算</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>

<blockquote>
  <p>帧差可说是最简单的一种背景模型，指定视频中的一幅图像为背景，用当前帧与背景进行比较，根据需要过滤较小的差异 （阈值），得到的结果就是前景了</p>
</blockquote>

<h3 id="2-背景减除法-background-subtraction">2. 背景减除法 (Background subtraction)</h3>

<p><strong>定义:</strong></p>

<p><strong>Background subtraction algorithm</strong> is to distinguish moving objects (hereafter referred to as the <code class="highlighter-rouge">foreground</code>) from static, or slow moving, parts
of the scene (called <code class="highlighter-rouge">background</code>).</p>

<blockquote>
  <p>Background subtraction, also known as <code class="highlighter-rouge">foreground detection(前景检测)</code>, is a technique in the fields of image processing and computer vision wherein an image’s foreground is extracted for further processing (object recognition etc.). Generally an image’s regions of interest are objects (humans, cars, text etc.) in its foreground. After the stage of image preprocessing (which may include image denoising, post processing like morphology etc.) object localisation is required which may make use of this technique.</p>

  <p>from <a href="https://en.wikipedia.org/wiki/Background_subtraction">wiki/Background_subtraction</a></p>
</blockquote>

<p><strong>需要解决的问题：</strong></p>

<ul>
  <li><strong>light changes(光照)</strong>: the background model should adapt to gradual or fast <code class="highlighter-rouge">illumination changes</code> (changing time of day, clouds, etc);</li>
  <li><strong>moving background</strong> or <strong>high frequency background objects</strong>(树叶等): the background model should include changing background that is not of interest for visual surveillance, such as <code class="highlighter-rouge">waving trees or branches</code>;</li>
  <li><strong>cast shadows(阴影)</strong>: the background model should include the <code class="highlighter-rouge">shadow cast by moving objects</code> that apparently behaves itself moving, in order to have a more accurate detection of the moving objects shape;</li>
  <li><strong>bootstrapping(初始化)</strong>: the background model should be properly set up even in the absence of a complete and static (free of moving objects) training set at the beginning of the sequence;</li>
  <li><strong>camouflage(背景相似)</strong>: moving objects should be detected even if their chromatic features are similar to those of the background model.</li>
  <li><strong>motion changes</strong> (camera oscillations);</li>
  <li>changes in the <strong>background geometry</strong> (e.g., parked cars).</li>
  <li><strong>Ghost</strong>区域：当一个原本静止的物体开始运动，背静差检测算法可能会将原来该物体所覆盖的区域错误的检测为运动的，这块区域就成为Ghost.</li>
</ul>

<p><strong>技术</strong>：
<code class="highlighter-rouge">Pixel-based</code> background subtraction: a static background frame, the (weighted) running average [21], first-order low-pass filtering [22], temporal median filtering [23], [24], and the modeling of each pixel with a Gaussian [25]–[27].</p>

<p>需要考虑三个问题：</p>

<ul>
  <li>如何 <strong>建立和使用</strong> 背景模型</li>
  <li>如何 <strong>初始化</strong> 背景模型</li>
  <li>如何 <strong>实时更新</strong> 背景模型</li>
</ul>

<h4 id="21-高斯模型-gaussian-model">2.1 高斯模型 (Gaussian Model)</h4>

<p><strong>Single Gussian &amp; Running Gaussian average</strong></p>

<ul>
  <li>Wren, Pfinder: <strong>Real-time tracking of the human body</strong>, <code class="highlighter-rouge">1997</code>, cited by <code class="highlighter-rouge">5000+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> – Pfinder is a real-time system for tracking people and interpreting thier behavior. It runs at 10Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multi-class statistical model of color and shape to obtain a 2-D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding.</p>
</blockquote>

<p><strong>Mixture of Gaussian Model</strong></p>

<ul>
  <li>KaewTraKulPong, <strong>An improved adaptive background mixture model for real-time tracking with shadow detection</strong>, <code class="highlighter-rouge">2001</code>, cited by <code class="highlighter-rouge">1400+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> – Real-time segmentation of moving regions in image sequences is a fundamental step in many vision systems including automated visual surveillance, human-machine interface, and very low-bandwidth telecommunications. A typical method is background subtraction. Many background models have been introduced to deal with different problems. One of the successful solutions to these problems is to use a multi-colour background model per pixel proposed by Grimson et al [1,2,3]. However, the method suffers
from slow learning at the beginning, especially in busy environments. In addition, it can not distinguish between moving shadows and moving objects. This paper presents a method which improves this adaptive background mixture model. By reinvestigating the update equations, we utilise different equations at different phases. This allows our system learn faster and more accurately as well as adapt effectively to changing environments. A shadow detection scheme is also introduced in this paper. It is based on a computational colour space that makes use of our background model. A comparison has been made
between the two algorithms. The results show the speed of learning and the accuracy of the model using our update algorithm over the Grimson et al’s tracker. When incorporate with the shadow detection, our method results in far better segmentation than that of Grimson et al.</p>
</blockquote>

<ul>
  <li>李鸿, <strong>基于混合高斯模型的运动检测及阴影消除算法研究</strong>, 中国民航大学硕士论文， <code class="highlighter-rouge">2013</code>, cited by <code class="highlighter-rouge">10</code></li>
  <li>卢章平，<strong>背景差分与三帧差分结合的运动目标检测算法</strong> ，计算机测量与控制，<code class="highlighter-rouge">2013</code>, cited by <code class="highlighter-rouge">44</code></li>
</ul>

<p><code class="highlighter-rouge">comment</code></p>
<blockquote>
  <p>混合高斯在现有的背景建模算法中应该算是比较好的，很多新的算法或改进的算法都是基于它的一些原理的不同变体，但混合高斯算法的缺点是计算量相对比较大，速度偏慢，对光照敏感</p>
</blockquote>

<h4 id="22-w4-algorithm-what-where-who-when">2.2 W4 algorithm (What? Where? Who? When?)</h4>

<ul>
  <li>Ismail Haritaoglu, <strong>W4: A Real Time System for Detecting and Tracking People</strong>, <code class="highlighter-rouge">1998</code>, cited by <code class="highlighter-rouge">1100+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em>  W^4 is a real time visual surveil lance system for detecting and tracking people and monitoring their activities in an outdoor environment. It operates on monocular grayscale video imagery, or on video imagery from an infrared camera. Unlike many of systems for tracking people, W^4 makes no use of color cues. Instead, W^4 employs a combination of shape analysis and tracking to locate people and their parts (head, hands, feet, torso) and to create models of people’s appearance so that they can be tracked through
interactions such as occlusions. W^4 is capable of simultaneously tracking multiple people even with occlusion. It runs at 25 Hz for 320x240 resolution images on a dual-pentium PC.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>
<blockquote>
  <p>W4算法应该是最早被用于实际应用的一个算法.</p>
</blockquote>

<h4 id="23-基于颜色信息的背景建模-color">2.3 基于颜色信息的背景建模 (color)</h4>

<ul>
  <li>Horprasert, <strong>A statistical approach for real-time robust background subtraction and shadow detection</strong>, <code class="highlighter-rouge">1999</code>, cited by <code class="highlighter-rouge">1200+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> This paper presents a novel algorithm for detecting moving objects from a static background scene that contains shading and shadows using color images. We develop a robust and efficiently computed background subtraction algorithm that is able to cope with local il lumination changes,
such as shadows and highlights, as wel l as global il lumination changes. The algorithm is based on a proposed computational color model which separates the brightness from the chromaticity component. We have applied this method to real image sequences of both indoor and outdoor scenes. The results, which demonstrate the system’s performance, and some speed up techniques we employed in our implementation are also shown.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>

<blockquote>
  <p>算法初衷：解决关于全局或局部的光照变化问题，例如阴影和高亮</p>

  <p>基于颜色信息的背景建模方法,简称Color算法，该算法将像素点的差异分解成Chromaticity差异和Brightness差异，对光照具有很强的鲁棒性，并有比较好的效果，计算速度也比较快，基本可以满足实时性的要求，做了许多视频序列的检测，效果比较理想；</p>
</blockquote>

<h4 id="24-本征背景法">2.4 本征背景法</h4>

<ul>
  <li>Nuria M. Oliver, <strong>A Bayesian computer vision system for modeling human interactions</strong>, <code class="highlighter-rouge">2000</code>, cited by <code class="highlighter-rouge">1500+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> — We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task [1]. The system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one’s path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both
components employing a statistical Bayesian approach [2]. We propose and compare two different state-based learning architectures, namely, HMMs and CHMMs for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. Finally, to deal with the problem of limited training data, a synthetic “Alife-style” training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>
<blockquote>
  <p>基于贝叶斯框架</p>
</blockquote>

<h4 id="25-核密度估计方法">2.5 核密度估计方法</h4>

<ul>
  <li>Ahmed Elgammal, <strong>Non-parametric model for background subtraction</strong>, <code class="highlighter-rouge">2000</code>, cited by <code class="highlighter-rouge">2500+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> Background subtraction is a method typically used to segment moving regions in image sequences taken from a static camera
by comparing each new frame to a model of the scene background. We
present a novel non-parametric background model and a background
subtraction approach. The model can handle situations where the background of the scene is cluttered and not completely static but contains
small motions such as tree branches and bushes. The model estimates
the probability of observing pixel intensity values based on a sample of
intensity values for each pixel. The model adapts quickly to changes in
the scene which enables very sensitive detection of moving targets. We
also show how the model can use color information to suppress detection of shadows. The implementation of the model runs in real-time for
both gray level and color imagery. Evaluation shows that this approach
achieves very sensitive detection with very low false alarm rates.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>
<blockquote>
  <p>比较鲁棒的算法，无需设置参数.</p>
</blockquote>

<h4 id="26-背景统计模型">2.6 背景统计模型</h4>

<blockquote>
  <p>对一段时间的背景进行统计，然后计算其统计数据（例如平均值、平均差分、标准差、均值漂移值等等），将统计数据作为背景的方法。</p>
</blockquote>

<p><strong>统计平均法</strong></p>

<ul>
  <li>BPL Lo, <strong>Automatic congestion detection system for underground platform</strong>, <code class="highlighter-rouge">2001</code>, cited by <code class="highlighter-rouge">300+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> - An automatic monitoring system is proposed in this paper for detecting overcrowding conditions in the platforms of underground train services.
Whenever overcrowding is detected, the system will notify the station operators to take appropriate actions to prevent accidents, such as
people falling off or being pushed onto the tracks. The system is designed to use existing closed circuit television (CCTV) cameras for acquiring
images of the platforms. In order to focus on the passengers on the platform, background subtraction and update techniques are used. In addition, due to the high variation of brightness on the platforms, a variance filter is introduced
to optimize the removal of background pixels. A multi-layer feed forward neural network was developed for classifying the levels of congestion. The system was tested with recorded video from the London Bridge station, and the testing results were shown to be accurate in identifying overcrowding conditions for the unique platform environment.</p>
</blockquote>

<p><strong>中值滤波法 (Temporal Median filter)</strong></p>

<ul>
  <li>R Cucchiara, <strong>Detecting Moving Objects, Ghosts, and Shadows in Video Streams</strong>, <code class="highlighter-rouge">2003</code>, cited by <code class="highlighter-rouge">1600+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> — Background subtraction methods are widely exploited for moving
object detection in videos in many applications, such as traffic monitoring, human motion capture, and video surveillance. How to correctly and efficiently model and update the background model and how to deal with shadows are two of the most distinguishing and challenging aspects of such approaches. This work proposes a general-purpose method that combines statistical assumptions with the objectlevel knowledge of moving objects, apparent objects (ghosts), and shadows acquired in the processing of the previous frames. Pixels belonging to moving objects, ghosts, and shadows are processed differently in order to supply an object-based selective update. The proposed approach exploits color information for both background subtraction and shadow detection to improve object segmentation and background update. The approach proves fast, flexible, and precise in terms of both pixel accuracy and reactivity to background changes.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>

<blockquote>

  <p>统计平均法和中值滤波法，算法的应用具有很大的局限性，只能算是理论上的一个补充.</p>
</blockquote>

<h4 id="27-复杂背景下的前景物体检测-fgd">2.7 复杂背景下的前景物体检测 (FGD)</h4>

<ul>
  <li>Liyuan Li, <strong>Foreground Object Detection from Videos Containing Complex Background</strong>, <code class="highlighter-rouge">2003</code>, cited by <code class="highlighter-rouge">500+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> –  This paper proposes a novel method for detection and segmentation of foreground objects from a video which contains both stationary and moving background objects and undergoes both gradual and sudden “once-off” changes. A Bayes decision rule for classification of background and foreground
from selected feature vectors is formulated. Under this rule, different types of background objects will be classified from foreground objects by choosing a proper feature vector. The stationary background object is described by the color feature, and the moving background object is represented by the color co-occurrence feature. Foreground objects are extracted by fusing the classification results from both stationary and moving pixels. Learning strategies for the gradual and sudden “once-off” background changes are proposed to adapt to various changes in background through the video. The convergence of the learning process is proved and a formula to select a proper learning rate is also derived. Experiments have shown promising results in extracting foreground objects from many complex backgrounds including wavering
tree branches, flickering screens and water surfaces, moving escalators, opening and closing doors, switching lights and shadows of moving objects.</p>
</blockquote>

<h4 id="28-码本-codebook">2.8 码本 (CodeBook)</h4>

<blockquote>
  <p>编码本的基本思路是这样的：针对每个像素在时间轴上的变动，建立多个（或者一个）包容近期所有变化的Box（变动范围）；在检测时，用当前像素与Box去比较，如果当前像素落在任何Box的范围内，则为背景。</p>
</blockquote>

<ul>
  <li>
    <p>K Kim, <strong>Real-time foreground–background segmentation using codebook model</strong>, <code class="highlighter-rouge">2005</code>, cited by <code class="highlighter-rouge">1400+</code></p>
  </li>
  <li>
    <p>A Ilyas, <strong>Real-time foreground-background segmentation using a modified codebook model</strong>, <code class="highlighter-rouge">2008</code>, cited by <code class="highlighter-rouge">50+</code></p>
  </li>
</ul>

<blockquote>
  <p><em>Abstract</em> – We present a real-time algorithm for foreground–background segmentation. Sample background values at each pixel are quantized into codebooks which represent a compressed form of background model for a long image sequence. This allows us to capture structural background variation due to periodic-like motion over a long period of time under limited memory. The
codebook representation is efficient in memory and speed compared with other background modeling techniques. Our method can handle scenes containing moving backgrounds or illumination variations, and it achieves robust detection for different types of videos. We compared our method with other multimode modeling techniques. 
In addition to the basic algorithm, two features improving the algorithm are presented—layered modeling/detection and adaptive
codebook updating.</p>
</blockquote>

<p><strong>Background modeling</strong></p>

<p>The CB algorithm adopts a <strong>quantization/clustering</strong> technique to construct a
background model from long observation sequences. For each pixel, it builds a <code class="highlighter-rouge">codebook</code> consisting of one or more codewords. Samples at each pixel are clustered into the set of codewords based on <code class="highlighter-rouge">a color distortion metric</code> together with brightness bounds. Not all pixels have the same number of codewords. The clusters represented by codewords do not necessarily correspond
to single Gaussian or other parametric distributions. Even if the distribution at a pixel were a single normal, there could be several codewords for that pixel. The background is encoded on a <code class="highlighter-rouge">pixel-by-pixel basis</code>.</p>

<p><strong>Detection</strong></p>

<p>Detection involves testing the difference of the current image from the background model with respect to <code class="highlighter-rouge">color and brightness differences</code>. If an incoming pixel meets two conditions, it is classified as background — (1) the color distortion to some codeword is less than the <code class="highlighter-rouge">detection threshold</code>, and (2) its brightness lies within the <code class="highlighter-rouge">brightness range</code> of that codeword. Otherwise, it is classified as foreground.</p>

<p><code class="highlighter-rouge">comment</code></p>
<blockquote>
  <p>效果还可以，有多种变体，对光照敏感</p>
</blockquote>

<h4 id="29-样本一致性背景建模算法--sacon">2.9 样本一致性背景建模算法  (SACON)</h4>

<ul>
  <li>Hanzi Wang, <strong>A consensus-based method for tracking: Modelling background scenario and foreground appearance</strong>, <code class="highlighter-rouge">2007</code>, cited by <code class="highlighter-rouge">100+</code>.</li>
</ul>

<blockquote>
  <p><em>Abstract</em> – Modelling of the background (“uninteresting parts of the scene”), and of the foreground, play important roles in the tasks of visual
detection and tracking of objects. This paper presents an effective and adaptive background modelling method for detecting foreground objects in both static and dynamic scenes. The proposed method computes SAmple CONsensus (SACON) of the background samples and estimates a statistical model of the background, per pixel. SACON exploits both color and motion information to detect foreground objects. SACON can deal with complex background scenarios including nonstationary scenes (such as moving trees, rain, and fountains),
moved/inserted background objects, slowly moving foreground objects, illumination changes etc. However, it is one thing to detect objects that are not likely to be part of the background; it is another task to track those objects. Sample consensus is again utilized to model the appearance of foreground objects to facilitate tracking. This appearance model is employed to
segment and track people through occlusions. Experimental results from several video sequences validate the effectiveness of the proposed method.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>

<blockquote>
  <p>基于统计知识，效果还可以</p>
</blockquote>

<h4 id="210-自组织背景建模--sobs-self-organization-background-subtraction">2.10 自组织背景建模  (SOBS: Self-organization background subtraction)</h4>

<ul>
  <li>Lucia Maddalena, <strong>A self-Organizing approach to background subtraction for visual surveillance Applications</strong>, <code class="highlighter-rouge">2008</code>, cited by <code class="highlighter-rouge">580+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> — Detection of moving objects in video streams is the first relevant step of information extraction in many computer vision applications. Aside from the intrinsic usefulness of being able to segment video streams into moving and background components, detecting moving objects provides a focus of attention for recognition, classification, and activity analysis, making these later steps more efficient. We propose an approach based on self organization through artificial neural networks, widely applied in human image processing systems and more generally in cognitive science. The proposed approach can handle scenes containing moving backgrounds, gradual illumination variations and camouflage, has no bootstrapping limitations, can include
into the background model shadows cast by moving objects, and achieves robust detection for different types of videos taken with stationary cameras. We compare our method with other modeling techniques and report experimental results, both in terms of detection accuracy and in terms of processing speed, for color video sequences that represent typical situations critical for video
surveillance systems.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>
<blockquote>
  <p>对光照有一定的鲁棒性，但MAP的模型比输入图片大，计算量比较大，但是可以通过并行处理来解决算法的速度问题，可以进行尝试</p>
</blockquote>

<h4 id="211-vibe-a-universal-background-subtraction">2.11 ViBe (A Universal Background Subtraction):</h4>

<ul>
  <li>Olivier Barnich, <strong>ViBe: A universal background subtraction algorithm for video sequences</strong>, <code class="highlighter-rouge">2011</code>, cited by <code class="highlighter-rouge">800+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> — This paper presents a technique for motion detection
that incorporates several innovative mechanisms. For example,
our proposed technique stores, for each pixel, a set of values taken
in the past at the same location or in the neighborhood. It then
compares this set to the current pixel value in order to determine
whether that pixel belongs to the background, and adapts the
model by choosing randomly which values to substitute from the
background model. This approach differs from those based on
the classical belief that the oldest values should be replaced first.
Finally, when the pixel is found to be part of the background, its
value is propagated into the background model of a neighboring
pixel. We describe our method in full details (including pseudocode and the parameter values used) and compare it to other
background subtraction techniques. Efficiency figures show that
our method outperforms recent and proven state-of-the-art
methods in terms of both computation speed and detection
rate. We also analyze the performance of a downscaled version
of our algorithm to the absolute minimum of one comparison
and one byte of memory per pixel. It appears that even such
a simplified version of our algorithm performs better than
mainstream techniques. An implementation of ViBe is available
at http://www.motiondetection.org.</p>
</blockquote>

<p><code class="highlighter-rouge">comment</code></p>

<blockquote>
  <p>VIBE算法是Barnich的一个大作，已申请了<code class="highlighter-rouge">专利</code>。</p>

  <p>ViBe是一种像素级视频背景建模或前景检测的算法。</p>

  <p>利用视频第一帧图像就能完成背景建模初始化工作，根据邻近像素点之间具有相似性完成初始化和更新，依据当前图像的像素和背景模型中对应像素之间的相似性程度来检测前景目标。</p>
</blockquote>

<p><strong>步骤：</strong>视频帧序列 -&gt; 第1帧 -&gt; 初始化背景模型 -&gt; 第2,…,N帧 -&gt; 前景目标检测-&gt;更新背景模型</p>

<p><strong>模型初始化</strong>： 为图像中每个像素建立一个大小为N的背景样本集，这个样本存储了该像素点邻近像素点的像素值以及过去这一点的像素值。</p>

<p><strong>像素分类/运动检测</strong>：判断像素是前景还是背景像素。当前帧与背景样本集比较，得到相似度，根据阈值判定。</p>

<p><strong>背景模型实时更新</strong>： 当前像素点被检测为背景像素，将按照一定概率用该像素点去更新自己的背景样本集或者是它的邻居点背景样本。</p>

<p><strong>算法的主要优势：</strong></p>
<ul>
  <li>内存占用少，没有浮点运算，计算量低，算法效率高，</li>
  <li>一个像素需要作一次比较，占用一个字节的内存；</li>
  <li>像素级算法，视频处理中的预处理关键步骤；</li>
  <li>背景模型初始化速度极快，适用于手持相机等复杂的视频环境；</li>
  <li>总体性能优于帧差发，光流法，混合高斯，SACON等，具有较好的抗噪能力。</li>
  <li>可直接应用在产品中，软硬件兼容性好；</li>
</ul>

<p><strong>算法的缺点：</strong></p>
<ul>
  <li>容易引入<strong>ghost</strong> 区域，“鬼影”。</li>
  <li>对 <strong>光照</strong> 强弱变化等动态场景敏感，不适用动态背景下的目标检测。</li>
  <li>无法消除运动目标的 <strong>阴影</strong> 。</li>
</ul>

<p><strong>改进：</strong></p>
<ul>
  <li>自适应阈值</li>
  <li>形态学处理</li>
  <li>结合三帧差分、边缘检测等技术</li>
</ul>

<p><code class="highlighter-rouge">More</code></p>

<ul>
  <li>M Van Droogenbroeck, <strong>Background subtraction: Experiments and improvements for ViBe</strong>, <code class="highlighter-rouge">2012</code>, cited by <code class="highlighter-rouge">140+</code></li>
  <li>余烨, <strong>EVibe:一种改进的Vibe运动目标检测算法</strong>,仪器仪表学报,2014, cited by <code class="highlighter-rouge">27</code></li>
</ul>

<blockquote>
  <p>此算法扩大了样本的取值范围，避免了样本的重复选取;采用隔行更新方式对邻域进行更新， 避免了错误分类的扩散;采用小目标丢弃和空洞填充策略去除了噪声的影响;添加了阴影去除模块, 增强了算法对阴影的鲁棒性</p>
</blockquote>

<ul>
  <li>胡小冉, <strong>一种新的基于ViBe的运动目标检测方法</strong>,计算机科学,<code class="highlighter-rouge">2014</code>, cited by <code class="highlighter-rouge">20</code></li>
</ul>

<blockquote>
  <p>预处理阶段通过三帧差分获得真实背景并消除鬼影，运动目标检测阶段结合先验知识和边缘检测方法获得真实的运动目标以消除阴影，目标描述与跟踪阶段运用像素标记分割方法得到目标描述 并实现目标跟踪。</p>
</blockquote>

<ul>
  <li>桂斌, <strong>基于ViBe的运动目标检测与阴影消除方法研究</strong>, 安徽大学硕士论文, <code class="highlighter-rouge">2015</code>, cited by <code class="highlighter-rouge">0</code></li>
  <li>王彬, <strong>基于改进的ViBE和HOG的运动目标检测系统研究与实现</strong>, 沈阳工业大学硕士论文,  <code class="highlighter-rouge">2016</code>, cited by <code class="highlighter-rouge">0</code></li>
</ul>

<h4 id="212-summary">2.12 Summary</h4>
<p>SOBS、Color、VIBE、SACON、W4等可以进行深入的了解，特别是近年来出现的Block-based或Region-Based、Features-Based、基于层次分类或层次训练器的算法可以进行深入的研究。</p>

<h3 id="3-运动分割motion-segmentation">3. 运动分割（motion segmentation）</h3>

<p>In <strong>motion segmentation</strong>, the moving objects are continuously present in the scene, and the background may also move due to camera motion. The target is <strong>to separate different motions</strong>.</p>

<h4 id="31-光流法-optical-flow">3.1 光流法 (optical flow)</h4>

<p>光流是一种可以观察到的目标的运行信息。当运动目标和摄像头发生相对运动，运动目标表明所携带的光学特征就能为我们带来目标的运动信息。光流就是运动目标在成像平面上像素点运动的随机速度。是非常<code class="highlighter-rouge">经典（古老）</code>基于运动的目标检测方法。</p>

<blockquote>
  <p>Optical flow or optic flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene.[1][2] The concept of optical flow was introduced by the American psychologist James J. Gibson in the 1940s to describe the visual stimulus provided to animals moving through the world.[3] Gibson stressed the importance of optic flow for affordance perception, the ability to discern possibilities for action within the environment. Followers of Gibson and his ecological approach to psychology have further demonstrated the role of the optical flow stimulus for the perception of movement by the observer in the world; perception of the shape, distance and movement of objects in the world; and the control of locomotion.[4]</p>

  <p>The term optical flow is also used by roboticists, encompassing related techniques from image processing and control of navigation including motion detection, object segmentation, time-to-contact information, focus of expansion calculations, luminance, motion compensated encoding, and stereo disparity measurement.[5][6]</p>

  <p>from <a href="https://en.wikipedia.org/wiki/Optical_flow">wikipedia/Optical_flow</a></p>

</blockquote>

<p>The dense optical flow is often used for <strong>Motion Segmentation(运动分割)</strong>.</p>

<ul>
  <li>David J. Fleet, <strong>Optical Flow Estimation</strong>, chapter15, <code class="highlighter-rouge">2005</code>, cited by <code class="highlighter-rouge">200+</code></li>
  <li>Stefan Roth, <strong>On the spatial statistics of optical flow</strong>, <code class="highlighter-rouge">2005</code>, cited by <code class="highlighter-rouge">260+</code></li>
  <li>董颖, <strong>基于光流场的视频运动检测</strong>, 山东大学硕士论文, <code class="highlighter-rouge">2008</code>, cited by <code class="highlighter-rouge">58</code>.</li>
  <li>裴巧娜，<strong>基于光流法的运动目标检测与跟踪技术</strong>，北方工业大学硕士论文，2009, cited by <code class="highlighter-rouge">107</code>.</li>
</ul>

<p>MathWorks: <a href="https://cn.mathworks.com/help/imaq/examples/live-motion-detection-using-optical-flow.html">Live Motion Detection Using Optical Flow</a></p>

<h4 id="32--运动竞争-motion-competition">3.2  运动竞争 (Motion Competition)</h4>

<ul>
  <li>Daniel Cremers, <strong>Motion Competition: A Variational Approach to Piecewise Parametric Motion Segmentation</strong>,<code class="highlighter-rouge">2005</code>, cited by <code class="highlighter-rouge">260+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> -  We present a novel variational approach for segmenting the image plane into a set of regions of parametric motion on the basis of two consecutive frames from an image sequence. Our model is based on a conditional probability for the spatio-temporal image gradient, given a particular velocity model, and on a geometric prior on the estimated motion field favoring motion boundaries of minimal length. Exploiting the Bayesian framework, we derive a cost functional which depends on parametric motion models for each of a set of regions and on the boundary separating these regions. The resulting functional can be interpreted as an extension of the Mumford-Shah functional from intensity segmentation to motion segmentation. In contrast to most alternative approaches, the problems of segmentation and motion estimation are jointly solved by continuous minimization of a single functional. Minimizing this functional with respect to its dynamic variables results in an eigenvalue problem for the motion parameters and in a gradient descent evolution for the motion discontinuity set. We propose two different representations of this motion boundary: an explicit spline-based implementation which can be applied to the motion-based tracking of a single moving object, and an implicit multiphase level set implementation which allows for the segmentation of an arbitrary number of multiply connected moving objects. Numerical results both for simulated ground truth experiments and for real-world sequences demonstrate the capacity of our approach to segment objects based exclusively on their relative motion.</p>
</blockquote>

<h4 id="33-decolor">3.3 DECOLOR</h4>

<p>DEtecting Contiguous Outliers in the LOw-rank Representation (DECOLOR)</p>

<ul>
  <li>Xiaowei zhou, <strong>Moving object detection by detecting contiguous outliers in the low-rank representation</strong>, <code class="highlighter-rouge">2013</code>, cited by <code class="highlighter-rouge">200+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> — Object detection is a fundamental step for automated video analysis in many vision applications. Object detection in a video
is usually performed by object detectors or background subtraction techniques. Often, an object detector requires manually labeled
examples to train a binary classifier, while background subtraction needs a training sequence that contains no objects to build a
background model. To automate the analysis, object detection without a separate training phase becomes a critical task. People have
tried to tackle this task by using motion information. But existing motion-based methods are usually limited when coping with complex
scenarios such as nonrigid motion and dynamic background. In this paper, we show that the above challenges can be addressed in a
unified framework named DEtecting Contiguous Outliers in the LOw-rank Representation (DECOLOR). This formulation integrates
object detection and background learning into a single process of optimization, which can be solved by an alternating algorithm
efficiently. We explain the relations between DECOLOR and other sparsity-based methods. Experiments on both simulated data and
real sequences demonstrate that DECOLOR outperforms the state-of-the-art approaches and it can work effectively on a wide range of
complex scenarios.</p>
</blockquote>

<blockquote>
  <p><em>Index Terms</em> — Moving object detection, low-rank modeling, Markov Random Fields, motion segmentation</p>
</blockquote>

<h4 id="34-long-term-video-analysis">3.4 Long Term Video Analysis</h4>

<ul>
  <li>Peter Ochs, <strong>Segmentation of Moving Objects by Long Term Video Analysis</strong>, <code class="highlighter-rouge">2014</code>, cited by <code class="highlighter-rouge">130+</code></li>
</ul>

<blockquote>
  <p><em>Abstract</em> — Motion is a strong cue for unsupervised object-level grouping. In this paper, we demonstrate that motion will be exploited
most effectively, if it is regarded over larger time windows. Opposed to classical two-frame optical flow, point trajectories that span
hundreds of frames are less susceptible to short-term variations that hinder separating different objects. As a positive side effect, the
resulting groupings are temporally consistent over a whole video shot, a property that requires tedious post-processing in the vast
majority of existing approaches. We suggest working with a paradigm that starts with semi-dense motion cues first and that fills up
textureless areas afterwards based on color. This paper also contributes the Freiburg-Berkeley motion segmentation (FBMS) dataset,
a large, heterogeneous benchmark with 59 sequences and pixel-accurate ground truth annotation of moving objects.</p>
</blockquote>

<blockquote>
  <p><em>Index Terms</em> — Motion segmentation, point trajectories, variational methods</p>
</blockquote>

<h3 id="4-其他方法">4. 其他方法</h3>

<h4 id="41-运动历史图像-motion-history-image-mhi">4.1 运动历史图像 （motion history image, MHI）</h4>

<ul>
  <li>James W. Davis, <strong>Hierarchical Motion History Images for Recognizing Human Motion</strong>, <code class="highlighter-rouge">2001</code>, cited by <code class="highlighter-rouge">170+</code></li>
  <li>MAR Ahad, <strong>Motion history image: its variants and applications</strong>, <code class="highlighter-rouge">2012</code>, cited by <code class="highlighter-rouge">170+</code></li>
</ul>

<blockquote>
  <p>The motion history image (MHI) is a static image template helps in understanding the motion location and path as it progresses.[1] In MHI, the temporal motion information is collapsed into a single image template where intensity is a function of recency of motion. Thus, the MHI pixel intensity is a function of the motion history at that location, where brighter values correspond to a more recent motion. Using MHI, moving parts of a video sequence can be engraved with a single image, from where one can predict the motion flow as well as the moving parts of the video action.</p>
</blockquote>

<blockquote>

  <p>Some important features of the MHI representation are:</p>

  <ul>
    <li>It represents motion sequence in a compact manner. In this case, the silhouette sequence is condensed into a grayscale image, where dominant motion information is preserved.</li>
    <li>MHI can be created and implemented in low illumination conditions where the structure cannot be easily detected otherwise.</li>
    <li>The MHI representation is not so sensitive to silhouette noises, holes, shadows, and missing parts.</li>
    <li>The gray-scale MHI is sensitive to the direction of motion because it can demonstrate the flow direction of the motion.</li>
    <li>It keeps a history of temporal changes at each pixel location, which then decays over time.</li>
    <li>The MHI expresses the motion flow or sequence by using the intensity of every pixel in a temporal manner.</li>
  </ul>

  <p>from <a href="https://en.wikipedia.org/wiki/Motion_History_Images">wikipedia/Motion_History_Images</a></p>
</blockquote>

<h2 id="recent-papers">Recent papers</h2>

<ol>
  <li>Pierre-Luc St-Charles, <strong>SuBSENSE: A Universal Change Detection Method With Local Adaptive Sensitivity</strong>, <code class="highlighter-rouge">2015</code>, cited by <code class="highlighter-rouge">80+</code></li>
</ol>

<h2 id="survey">Survey</h2>
<ol>
  <li>W Hu, <strong>A survey on visual surveillance of object motion and behaviors</strong>, <code class="highlighter-rouge">2004</code>, cited by <code class="highlighter-rouge">2300+</code></li>
  <li>M Piccardi, <strong>Background subtraction techniques: A review</strong>, <code class="highlighter-rouge">2004</code>, cited by <code class="highlighter-rouge">1900+</code></li>
  <li>Thomas B. Moeslund, <strong>A survey of advances in vision-based human motion capture and analysis</strong>, <code class="highlighter-rouge">2006</code>, cited by <code class="highlighter-rouge">2400+</code></li>
  <li>S Brutzer, <strong>Evaluation of Background Subtraction Techniques for Video Surveillance</strong>, <code class="highlighter-rouge">2011</code>, cited by <code class="highlighter-rouge">400+</code></li>
  <li>A Sobral, <strong>A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos</strong>, <code class="highlighter-rouge">2014</code>, cited by <code class="highlighter-rouge">200+</code></li>
  <li>T Bouwmans, <strong>Traditional and recent approaches in background modeling for foreground detection  An overview</strong>, <code class="highlighter-rouge">2014</code>, cited by <code class="highlighter-rouge">180+</code></li>
</ol>

<h2 id="librarysoftware">Library/Software</h2>

<p><strong>Background subtraction Library</strong></p>
<ol>
  <li><a href="https://github.com/andrewssobral/bgslibrary"><strong>BGSLibrary</strong></a>: The BGS Library (A. Sobral, Univ. La Rochelle, France) provides a C++ framework to perform background subtraction algorithms. The code works either on Windows or on Linux. Currently the library offers more than 30 BGS algorithms.</li>
  <li><a href="https://github.com/andrewssobral/lrslibrary"><strong>LRS Library</strong></a> - Low-Rank and Sparse tools for Background Modeling and Subtraction in Videos. The LRSLibrary (A. Sobral, Univ. La Rochelle, France) provides a collection of low-rank and sparse decomposition algorithms in MATLAB. The library was designed for motion segmentation in videos, but it can be also used or adapted for other computer vision problems. Currently the LRSLibrary contains more than 100 matrix-based and tensor-based algorithms.</li>
</ol>

<p><strong>Other libary</strong></p>
<ol>
  <li><a href="https://www.codeproject.com/Articles/10248/Motion-Detection-Algorithms"><strong>Motion Detection Algorithms</strong></a>: There are many approaches for motion detection in a continuous video stream. All of them are based on comparing of the current video frame with one from the previous frames or with something that we’ll call background. In this article, I’ll try to describe some of the most common approaches.</li>
</ol>

<h2 id="reference">Reference</h2>

<ol>
  <li>目标检测中背景建模方法 <a href="http://www.cnblogs.com/ronny/archive/2012/04/12/2444053.html">http://www.cnblogs.com/ronny/archive/2012/04/12/2444053.html</a></li>
  <li>A video database for testing change detection algorithms <a href="http://www.changedetection.net/">http://www.changedetection.net/</a></li>
</ol>


                <hr style="visibility: hidden;">

				
				
				<div id="disqus_thread"></div>
				<script>

				/**
				*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
				*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
				/*
				var disqus_config = function () {
				this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
				this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
				};
				*/
				var disqus_username = 'huajh7';
				
				(function() { // DON'T EDIT BELOW THIS LINE
				var d = document, s = d.createElement('script');
				s.src = 'https://'+disqus_username+'.disqus.com/embed.js';
				s.setAttribute('data-timestamp', +new Date());
				(d.head || d.body).appendChild(s);
				})();
				</script>
				<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
				
				
				
				

                <!-- 网易云跟帖 start -->
                
                <!-- 网易云跟帖  end -->
							

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2017/04/04/markdown_maxiang/" data-toggle="tooltip" data-placement="top" title="Markdown简介">
                        Previous<br>
                        <span>Markdown简介</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2017/04/08/Image-dehazing/" data-toggle="tooltip" data-placement="top" title="除雾算法最新进展">
                        Next<br>
                        <span>除雾算法最新进展</span>
                        </a>
                    </li>
                    
                </ul>





            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
                				<a href="/tags/#Machine Learning" title="Machine Learning" rel="2">
                                    Machine Learning
                                </a>
                            
        				
                            
                				<a href="/tags/#Variational Inference" title="Variational Inference" rel="2">
                                    Variational Inference
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
                				<a href="/tags/#Computer Vision" title="Computer Vision" rel="2">
                                    Computer Vision
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="http://www.huajh7.com">My academic Website</a></li>
                    
                        <li><a href="http://www.homfen.me">Homfen</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>










<!-- disqus 公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "huajh7";
    var disqus_identifier = "/2017/04/06/motion-detection";
    var disqus_url = "http://localhost:4000/2017/04/06/motion-detection/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>





    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    

                    
                    <li>
                        <a target="_blank" href="http://weibo.com/huajh7">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    

                    
                    <li>
                        <a target="_blank" href="https://github.com/huajh">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    

                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/hua-jun-hao">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                    


                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/junhao-hua-01b03150">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    

                                        
                </ul>
                <p class="copyright text-muted" style="font-size: 17px">
                    Copyright &copy; huajh7's Blog 2017
                    <br>
                    <a href="http://www.easycounter.com/">
                    <img alt="Website Hit Counter" src="http://www.easycounter.com/counter.php?blog_huajh7" border="0"></a>
                     <i><font size="2" face="Arial">Vistors Since Apr 2017</font></i>
                    <br>
<!--                     Theme by <a href="https://github.com/Huxpro/huxpro.github.io">Hux</a> -->
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script type="text/javascript">
    if(navigator.serviceWorker){
        // For security reasons, a service worker can only control the pages that are in the same directory level or below it. That's why we put sw.js at ROOT level.
        navigator.serviceWorker
            .register('/sw.js')
            .then((registration) => {console.log('Service Worker Registered. ', registration)})
            .catch((error) => {console.log('ServiceWorker registration failed: ', error)})
    }
</script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/ 
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers   
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async('/js/jquery.tagcloud.js',function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = '78399d5a8a7f58e0d22390246d356dab';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>




<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {
        var P = $('div.post-container'),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;    
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>





<!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0" /> -->
<!-- Migrate from head to bottom, no longer block render and still work -->

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>

</html>
